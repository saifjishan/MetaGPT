import os
import json
import asyncio
import logging
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from fastapi.responses import JSONResponse
import socketio
import uvicorn
import sys

# Add the parent directory to sys.path to import MetaGPT modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# Import MetaGPT modules
from metagpt.logs import logger
from metagpt.config import CONFIG
from metagpt.roles import Role
from metagpt.team import Team
from metagpt.actions import Action
from metagpt.schema import Message
from metagpt.provider.base_llm import BaseLLM
from metagpt.provider.openai_api import OpenAILLM
from metagpt.provider.xai_api import XAILLM
from metagpt.provider.gemini_api import GeminiLLM
from metagpt.configs.llm_config import LLMType

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(title="MetaGPT API", version="1.0.0")

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Create Socket.IO server
sio = socketio.AsyncServer(async_mode="asgi", cors_allowed_origins="*")
socket_app = socketio.ASGIApp(sio)
app.mount("/socket.io", socket_app)

# Connected WebSocket clients
connected_clients = set()

# Models
class APIKeyConfig(BaseModel):
    openai_api_key: Optional[str] = None
    xai_api_key: Optional[str] = None
    gemini_api_key: Optional[str] = None

class ProjectRequest(BaseModel):
    prompt: str
    llm_type: str = "OPENAI"  # OPENAI, XAI, GEMINI
    api_keys: Optional[APIKeyConfig] = None

class AgentMessage(BaseModel):
    role_name: str
    content: str
    thinking: Optional[str] = None

class ProjectResponse(BaseModel):
    id: str
    status: str
    messages: List[AgentMessage] = []

# In-memory storage for projects
projects_db = {}

# LLM provider mapping
llm_providers = {
    "OPENAI": OpenAILLM,
    "XAI": XAILLM,
    "GEMINI": GeminiLLM
}

# Custom MetaGPT observer to capture agent messages
class WebSocketObserver:
    def __init__(self, project_id: str):
        self.project_id = project_id
        
    async def on_message(self, message: Message):
        """Called when a new message is generated by an agent"""
        if not message.content:
            return
            
        agent_message = AgentMessage(
            role_name=message.role_name or "System",
            content=message.content,
            thinking=message.instruct_content
        )
        
        if self.project_id in projects_db:
            projects_db[self.project_id]["messages"].append(agent_message.dict())
            
            # Broadcast to all connected clients
            await sio.emit(
                "agent_message", 
                {
                    "project_id": self.project_id,
                    "message": agent_message.dict()
                }
            )

# WebSocket connection
@sio.event
async def connect(sid, environ):
    logger.info(f"Client connected: {sid}")
    connected_clients.add(sid)
    await sio.emit("connection_status", {"status": "connected", "client_id": sid})

@sio.event
async def disconnect(sid):
    logger.info(f"Client disconnected: {sid}")
    connected_clients.remove(sid)

# API endpoints
@app.get("/")
async def root():
    return {"message": "MetaGPT API is running"}

@app.post("/api/projects", response_model=ProjectResponse)
async def create_project(request: ProjectRequest):
    """Create a new project with the given prompt"""
    try:
        # Generate a unique project ID
        import uuid
        project_id = str(uuid.uuid4())
        
        # Configure API keys if provided
        if request.api_keys:
            if request.api_keys.openai_api_key:
                os.environ["OPENAI_API_KEY"] = request.api_keys.openai_api_key
            if request.api_keys.xai_api_key:
                os.environ["XAI_API_KEY"] = request.api_keys.xai_api_key
            if request.api_keys.gemini_api_key:
                os.environ["GEMINI_API_KEY"] = request.api_keys.gemini_api_key
        
        # Create a new project
        projects_db[project_id] = {
            "id": project_id,
            "prompt": request.prompt,
            "status": "created",
            "messages": []
        }
        
        # Start the project in a background task
        asyncio.create_task(run_metagpt(project_id, request.prompt, request.llm_type))
        
        return ProjectResponse(
            id=project_id,
            status="created",
            messages=[]
        )
    except Exception as e:
        logger.error(f"Error creating project: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/projects/{project_id}", response_model=ProjectResponse)
async def get_project(project_id: str):
    """Get a project by ID"""
    if project_id not in projects_db:
        raise HTTPException(status_code=404, detail="Project not found")
    
    return ProjectResponse(
        id=project_id,
        status=projects_db[project_id]["status"],
        messages=[AgentMessage(**msg) for msg in projects_db[project_id]["messages"]]
    )

@app.get("/api/projects")
async def list_projects():
    """List all projects"""
    return {"projects": [{"id": pid, "status": data["status"]} for pid, data in projects_db.items()]}

@app.get("/api/config")
async def get_config():
    """Get the current configuration"""
    return {
        "llm_providers": list(llm_providers.keys()),
        "has_openai_key": bool(os.environ.get("OPENAI_API_KEY")),
        "has_xai_key": bool(os.environ.get("XAI_API_KEY")),
        "has_gemini_key": bool(os.environ.get("GEMINI_API_KEY")),
    }

# MetaGPT execution
async def run_metagpt(project_id: str, prompt: str, llm_type: str):
    """Run MetaGPT with the given prompt"""
    try:
        # Update project status
        projects_db[project_id]["status"] = "running"
        
        # Create WebSocket observer
        observer = WebSocketObserver(project_id)
        
        # Select LLM provider
        if llm_type not in llm_providers:
            raise ValueError(f"Unsupported LLM type: {llm_type}")
        
        llm_provider = llm_providers[llm_type]
        llm_config = CONFIG.llm
        
        # Create a team with the selected LLM
        from metagpt.roles import ProductManager, Architect, ProjectManager, Engineer, QaEngineer
        
        # Create the team with roles
        team = Team()
        team.hire([
            ProductManager(),
            Architect(),
            ProjectManager(),
            Engineer(name="Engineer 1"),
            Engineer(name="Engineer 2"),
            QaEngineer()
        ])
        
        # Set the observer for each role
        for role in team.members:
            role.subscribe(observer)
        
        # Start the team with the prompt
        await team.run(prompt)
        
        # Update project status
        projects_db[project_id]["status"] = "completed"
        
    except Exception as e:
        logger.error(f"Error running MetaGPT: {str(e)}")
        projects_db[project_id]["status"] = "failed"
        projects_db[project_id]["error"] = str(e)
        
        # Notify clients of the error
        await sio.emit(
            "project_error",
            {
                "project_id": project_id,
                "error": str(e)
            }
        )

if __name__ == "__main__":
    uvicorn.run("app:app", host="0.0.0.0", port=12001, reload=True)